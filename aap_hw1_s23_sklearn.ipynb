{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b2bda9-61ef-45ea-a67b-4bedba8f63be",
   "metadata": {},
   "source": [
    "## Preliminary Data Prep\n",
    "In order to use this data for a classification problem, I did some data prep work. Our target variable is a new variable that I created called `price_gt_1M` which is a binary variable:\n",
    "\n",
    "* 1 - house price is greater than or equal to 1 million dollars\n",
    "* 0 - house price is less than a million dollars\n",
    "\n",
    "The data for this classification problem can be found in `./data/kc_house_data_classification.csv`.\n",
    "\n",
    "If you want to see my data prep code, see the `hw1_sklearn_dataprep.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a3b77-cd10-454e-bb65-cebbfb51ae3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Problem\n",
    "\n",
    "Our overall goal is to build classifier models to predict `price_gt_1M` using the the other variables. You must use sklearn Pipelines that contain your preprocessing steps and your model estimation step. We did this in the class notes.\n",
    "\n",
    "You should do your work in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78147e4-e1c0-4175-ab38-c05b52514037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Task 1 - Folder structure\n",
    "\n",
    "Start by creating a new project folder structure with the `cookiecutter-datascience-simple` template that I covered in Module 1. Put the data files into an appropriate folder and put this notebook in the main project folder. Any additional notebooks and/or Python files you end up creating should go in the main project folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ea0e1-f002-4cc3-a8b3-847872f89d01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Task 2 - Version control\n",
    "\n",
    "Put your new project folder under version control using git. You should **NOT** track the data file. You must track all notebooks, Python scripts or additional text files you end up creating. Put appropriate information into your readme file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef43a51-cc77-4fb0-863e-ec3f4a74a646",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3 - EDA\n",
    "\n",
    "I suggest you start by reading the csv file into a pandas dataframe. I called my dataframe, `housing_df`.\n",
    "Then start with some basic EDA. You can certainly use automated tools such as pandas-profiling, skimpy or SweetViz as I showed in the class notes. Remember, when you run some of those tools, you **must** have your notebook open in the classic Jupyter Notebook interface (and **NOT** in Jupyter Lab) Check their docs to see if Jupyter Lab is supported yet. I pip installed SweetViz and it seems to be working fine now with Jupyter Lab. As we've seen, the reports get created as HTML documents. These should go in your output folder within your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d238426e-b082-420e-8fb8-07afd7f707bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f799cb59-37ab-48e7-893f-c24714720f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aa126314-26f7-42da-9db9-7809ee50a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_house_df = pd.read_csv(\"data/kc_house_data_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf836c0-2bd2-49e7-afd0-381870f61e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/dataframe_report.py:74: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  all_source_names = [cur_name for cur_name, cur_series in source_df.iteritems()]\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/dataframe_report.py:109: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  filtered_series_names_in_source = [cur_name for cur_name, cur_series in source_df.iteritems()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84834d43700a430c91e0e4d273cb1440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                             |      | [  0%]   00:00 -> (? left)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_cat.py:28: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in category_counts.iteritems():\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_cat.py:28: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in category_counts.iteritems():\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_cat.py:28: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in category_counts.iteritems():\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_cat.py:28: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in category_counts.iteritems():\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_numeric.py:25: FutureWarning: The 'mad' method is deprecated and will be removed in a future version. To compute the same result, you may do `(df - df.mean()).abs().mean()`.\n",
      "  stats[\"mad\"] = series.mad()\n",
      "/Users/anthonystronati/anaconda3/lib/python3.10/site-packages/sweetviz/series_analyzer_cat.py:28: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for item in category_counts.iteritems():\n"
     ]
    }
   ],
   "source": [
    "report = sv.analyze(kc_house_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1906a8bd-b5f7-4ed7-b9d9-69d1aca07a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report output/sweetviz_report.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files.\n"
     ]
    }
   ],
   "source": [
    "report.show_html(\"output/sweetviz_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ebd035-5afa-48e0-8a7e-e25b7bbe28b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4 - Categorize feature types\n",
    "\n",
    "As we did in the Pump it Up class notes, we are going to need to create a list of categorical variables and a list of numeric variables so that we\n",
    "can apply the appropriate pre-processing to each. In the notes we used the data type of the columns to create lists of numeric and categorical variables. That's not necessarily going to work here as all the variables will come in as numeric. So, you'll have to come up with another way to create lists of the categorical variables and the numeric variables. \n",
    "\n",
    "Since we are using regularization, all of the numeric variables will need to rescaled using the `StandardScaler`. You'll do this later as part of the `Pipeline`. For any variables that you decide should be treated as categorical in your models, use the `OneHotEncoder` on them in the preprocessing stage.\n",
    "\n",
    "Be careful, just because a variable has a numeric datatype in the pandas dataframe, it does **not** mean that it's necessarily a numeric variable in the context of the classification models. Think about each column and look at your EDA reports and decide whether or not it's truly numeric or needs to be treated as categorical data in the models.  \n",
    "\n",
    "Even though our target variable, `price_gt_1M`, is categorical, you do **NOT** need to do any preprocessing on it. As I mentioned in our class notes, scikit-learn will automatically detect that and will do any encoding needed on its own.\n",
    "\n",
    "Finally, you'll partition the dataset into training and test datasets for modeling: \n",
    "\n",
    "* I broke up the `housing_df` into two separate dataframes that I called `X` and `y`, to use in the models. Here's my code for that:\n",
    "\n",
    "```\n",
    "X = housing_df.iloc[:, 0:18]\n",
    "y = housing_df.iloc[:, 18]\n",
    "```\n",
    "\n",
    "* Please use the following code for your data partitioning so that we all end up with the same training and test split:\n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "69f4465b-d9d0-434e-8989-69883892c11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bedrooms           int64\n",
       "bathrooms        float64\n",
       "sqft_living        int64\n",
       "sqft_lot           int64\n",
       "floors           float64\n",
       "waterfront         int64\n",
       "view               int64\n",
       "condition          int64\n",
       "grade              int64\n",
       "sqft_above         int64\n",
       "sqft_basement      int64\n",
       "yr_built           int64\n",
       "yr_renovated       int64\n",
       "zipcode            int64\n",
       "lat              float64\n",
       "long             float64\n",
       "sqft_living15      int64\n",
       "sqft_lot15         int64\n",
       "price_gt_1M        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_house_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "75066e56-e511-4184-a2c5-bc3e1bb8862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a limit for the number of unique values\n",
    "unique_limit = 30 # I just keep changing this number until it gave me a list of catigorical I liked. \n",
    "\n",
    "# create variables\n",
    "categorical_vars = []\n",
    "numeric_vars = []\n",
    "\n",
    "# loop through all columns\n",
    "for column in kc_house_df.columns:\n",
    "    unique_values = kc_house_df[column].nunique()  # Count the number of unique values\n",
    "    \n",
    "    if unique_values <= unique_limit:\n",
    "        categorical_vars.append(column)\n",
    "    else:\n",
    "        numeric_vars.append(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "60a6e67f-acdd-4fc7-83f0-6998040760bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bedrooms',\n",
       " 'bathrooms',\n",
       " 'floors',\n",
       " 'waterfront',\n",
       " 'view',\n",
       " 'condition',\n",
       " 'grade',\n",
       " 'price_gt_1M']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f7534a37-84d4-4a12-ac79-920abe121897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sqft_living',\n",
       " 'sqft_lot',\n",
       " 'sqft_above',\n",
       " 'sqft_basement',\n",
       " 'yr_built',\n",
       " 'yr_renovated',\n",
       " 'zipcode',\n",
       " 'lat',\n",
       " 'long',\n",
       " 'sqft_living15',\n",
       " 'sqft_lot15']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b6e47fb4-b563-4b20-a1a9-8fbeffe0fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_vars = ['bedrooms','bathrooms','floors','waterfront','view','condition','grade','price_gt_1M','yr_built','yr_renovated','zipcode']\n",
    "# numeric_vars = ['sqft_living','sqft_lot','sqft_above','sqft_basement','lat','long','sqft_living15','sqft_lot15']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "696dddcf-d390-49b0-997a-d48075a3d91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>price_gt_1M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.370842</td>\n",
       "      <td>2.114757</td>\n",
       "      <td>1.494309</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>0.234303</td>\n",
       "      <td>3.409430</td>\n",
       "      <td>7.656873</td>\n",
       "      <td>0.069033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.930062</td>\n",
       "      <td>0.770163</td>\n",
       "      <td>0.539989</td>\n",
       "      <td>0.086517</td>\n",
       "      <td>0.766318</td>\n",
       "      <td>0.650743</td>\n",
       "      <td>1.175459</td>\n",
       "      <td>0.253515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bedrooms     bathrooms        floors    waterfront          view  \\\n",
       "count  21613.000000  21613.000000  21613.000000  21613.000000  21613.000000   \n",
       "mean       3.370842      2.114757      1.494309      0.007542      0.234303   \n",
       "std        0.930062      0.770163      0.539989      0.086517      0.766318   \n",
       "min        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "25%        3.000000      1.750000      1.000000      0.000000      0.000000   \n",
       "50%        3.000000      2.250000      1.500000      0.000000      0.000000   \n",
       "75%        4.000000      2.500000      2.000000      0.000000      0.000000   \n",
       "max       33.000000      8.000000      3.500000      1.000000      4.000000   \n",
       "\n",
       "          condition         grade   price_gt_1M  \n",
       "count  21613.000000  21613.000000  21613.000000  \n",
       "mean       3.409430      7.656873      0.069033  \n",
       "std        0.650743      1.175459      0.253515  \n",
       "min        1.000000      1.000000      0.000000  \n",
       "25%        3.000000      7.000000      0.000000  \n",
       "50%        3.000000      7.000000      0.000000  \n",
       "75%        4.000000      8.000000      0.000000  \n",
       "max        5.000000     13.000000      1.000000  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_house_df.loc[:, categorical_vars].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "71b3c9f9-d96e-459b-b68b-75030ab6e7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21613.000000</td>\n",
       "      <td>2.161300e+04</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "      <td>21613.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2079.899736</td>\n",
       "      <td>1.510697e+04</td>\n",
       "      <td>1788.390691</td>\n",
       "      <td>291.509045</td>\n",
       "      <td>1971.005136</td>\n",
       "      <td>84.402258</td>\n",
       "      <td>98077.939805</td>\n",
       "      <td>47.560053</td>\n",
       "      <td>-122.213896</td>\n",
       "      <td>1986.552492</td>\n",
       "      <td>12768.455652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>918.440897</td>\n",
       "      <td>4.142051e+04</td>\n",
       "      <td>828.090978</td>\n",
       "      <td>442.575043</td>\n",
       "      <td>29.373411</td>\n",
       "      <td>401.679240</td>\n",
       "      <td>53.505026</td>\n",
       "      <td>0.138564</td>\n",
       "      <td>0.140828</td>\n",
       "      <td>685.391304</td>\n",
       "      <td>27304.179631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>5.200000e+02</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.519000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1427.000000</td>\n",
       "      <td>5.040000e+03</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1951.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.471000</td>\n",
       "      <td>-122.328000</td>\n",
       "      <td>1490.000000</td>\n",
       "      <td>5100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1910.000000</td>\n",
       "      <td>7.618000e+03</td>\n",
       "      <td>1560.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1975.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98065.000000</td>\n",
       "      <td>47.571800</td>\n",
       "      <td>-122.230000</td>\n",
       "      <td>1840.000000</td>\n",
       "      <td>7620.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2550.000000</td>\n",
       "      <td>1.068800e+04</td>\n",
       "      <td>2210.000000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.678000</td>\n",
       "      <td>-122.125000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10083.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13540.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>9410.000000</td>\n",
       "      <td>4820.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>6210.000000</td>\n",
       "      <td>871200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sqft_living      sqft_lot    sqft_above  sqft_basement      yr_built  \\\n",
       "count  21613.000000  2.161300e+04  21613.000000   21613.000000  21613.000000   \n",
       "mean    2079.899736  1.510697e+04   1788.390691     291.509045   1971.005136   \n",
       "std      918.440897  4.142051e+04    828.090978     442.575043     29.373411   \n",
       "min      290.000000  5.200000e+02    290.000000       0.000000   1900.000000   \n",
       "25%     1427.000000  5.040000e+03   1190.000000       0.000000   1951.000000   \n",
       "50%     1910.000000  7.618000e+03   1560.000000       0.000000   1975.000000   \n",
       "75%     2550.000000  1.068800e+04   2210.000000     560.000000   1997.000000   \n",
       "max    13540.000000  1.651359e+06   9410.000000    4820.000000   2015.000000   \n",
       "\n",
       "       yr_renovated       zipcode           lat          long  sqft_living15  \\\n",
       "count  21613.000000  21613.000000  21613.000000  21613.000000   21613.000000   \n",
       "mean      84.402258  98077.939805     47.560053   -122.213896    1986.552492   \n",
       "std      401.679240     53.505026      0.138564      0.140828     685.391304   \n",
       "min        0.000000  98001.000000     47.155900   -122.519000     399.000000   \n",
       "25%        0.000000  98033.000000     47.471000   -122.328000    1490.000000   \n",
       "50%        0.000000  98065.000000     47.571800   -122.230000    1840.000000   \n",
       "75%        0.000000  98118.000000     47.678000   -122.125000    2360.000000   \n",
       "max     2015.000000  98199.000000     47.777600   -121.315000    6210.000000   \n",
       "\n",
       "          sqft_lot15  \n",
       "count   21613.000000  \n",
       "mean    12768.455652  \n",
       "std     27304.179631  \n",
       "min       651.000000  \n",
       "25%      5100.000000  \n",
       "50%      7620.000000  \n",
       "75%     10083.000000  \n",
       "max    871200.000000  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_house_df.loc[:, numeric_vars].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cb5c1f20-0c9b-4691-90cb-90c5e3f85c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 5, 6, 7, 8, 18]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_vars_idx = [kc_house_df.columns.get_loc(c) for c in categorical_vars]\n",
    "categorical_vars_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0f1c8d91-8769-4fa3-94f1-0a3c5b05bb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bedrooms       13\n",
       "bathrooms      30\n",
       "floors          6\n",
       "waterfront      2\n",
       "view            5\n",
       "condition       5\n",
       "grade          12\n",
       "price_gt_1M     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_house_df.iloc[:, categorical_vars_idx].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4311bdf8-e699-4511-b2bd-a148c112657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a4f08d0f-3ccd-414a-a6b4-ddcbbb6568a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer objects\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor step\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_vars),\n",
    "        ('cat', categorical_transformer, categorical_vars)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "51f7ecec-d61e-4d72-ad44-1464d82e2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separte predictors and target\n",
    "X = kc_house_df.iloc[:, 0:18]\n",
    "y = kc_house_df.iloc[:, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1fd8d376-9262-4dc3-a9a1-0178a9cc9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition training data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "edff0ab2-ac78-448b-ae9f-911e7e1daab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.info of 13067    0\n",
       "17417    0\n",
       "1290     0\n",
       "13137    0\n",
       "15815    0\n",
       "        ..\n",
       "13704    0\n",
       "16144    0\n",
       "8586     0\n",
       "4014     0\n",
       "21398    0\n",
       "Name: price_gt_1M, Length: 17290, dtype: int64>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb25f3-5980-4ddd-9c60-baba45ea6449",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 4 - Logistic regression models\n",
    "\n",
    "You are going to build a few different logistic regression models using all of the variables in our housing dataset. For each of these models you will:\n",
    "\n",
    "- Create a pipeline to do the preprocessing (the scaling and encoding) and the modeling (we did this in the Pump it Up project)\n",
    "- I'll be giving you different specifications and hyperparameter parameter settings to try\n",
    "- You'll be scoring the models on overall accuracy for both the training and test data. Discuss any evidence of overfitting or underfitting as well as how the model does in comparison to the null model.\n",
    "- There will be some additional tasks/questions for each model - details below\n",
    "\n",
    "**IMPORTANT** You always should put summary comments in one or more markdown cells. Do **NOT** write them as comments in a code cell. The whole point of Jupyter notebooks is to be able to mix markdown cells with code cells. Yes, you should also include code comments but those are different than analysis comments.\n",
    "\n",
    "#### Model 0: The null model\n",
    "\n",
    "We always start with the simplest possible model and we call it the *null model*. For binary classification models, the null model is usually just to predict that each observation will fall into whichever class is most prevalent. In other words, what would be the performance of a model in which we just predict 0 for everyone? Scikit-learn has a way to create simple null models for classification with the `sklearn.dummy.DummyClassifier` class. See https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html for the details. You must use this class to create your null model.\n",
    "\n",
    "#### Model 1: Ridge regression with C=1.0\n",
    "\n",
    "Build a ridge regression model to predict `price_gt_1M` and use the default value of `C=1.0`. I used the following additional options with the `LogisticRegression` model - `solver='saga', max_iter=2000`. Feel free to change these if you want. AFter fitting the model, compute its accuracy score for training and test and write out a little summary (f-strings are useful). Here's an example:\n",
    "\n",
    "    Training score: 0.974\n",
    "    Test score: 0.971\n",
    "\n",
    "Create confusion matrices for both training and test.\n",
    "\n",
    "Also, create a plot of the coefficients (as we did in the notes). If you want to use that `coef_plot` function we used in the notes, you'll have\n",
    "to make a few modifications because we only have one set of coefficients (since we have a binary classification problem as opposed to a 3-class problem in Pump it Up).\n",
    "\n",
    "#### Model 2: Lasso regression with C=1.0\n",
    "\n",
    "Same as Model 1, but use lasso regression instead of ridge regression. Create the same outputs and compare the performance to the ridge regression model.\n",
    "\n",
    "#### Model 3: Lasso regression with C=0.01\n",
    "\n",
    "Fit another lasso regression but use `C=0.01`. Does this enforce more or less regularization? Create the same outputs and compare the performance to the first two models. Discuss why the plot looks so different than the previous plots.\n",
    "\n",
    "#### Model 4: Lasso regression with optimal C value\n",
    "\n",
    "Now use `LogisticRegressionCV` to fit a model and let sklearn determine the optimal `C` value to use. Again, compute the accuracy score and confusion matrices. Also, print out the optimal value of `C`. Does regularization help for this problem?\n",
    "\n",
    "### Task 5 - Simple decision tree\n",
    "Now fit a decision tree to predict `price_gt_1M`. As we did above, for both train and test, compute the accuracy score, create a confusion matrix, and discuss the performance relative to your logistic regression models. Obviously you do not need to create a coefficient plot (why not?). \n",
    "\n",
    "**HACKER EXTRA:** See if you can figure out how to display the fitted tree so that it's readable. :)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "12756feb-5d36-4997-9b34-cd2919492f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c79435d3-14b0-4558-bd31-c7621f8826f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer objects\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Combine transformers into a preprocessor step\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_vars),\n",
    "        ('cat', categorical_transformer, categorical_vars)])\n",
    "\n",
    "# Classifier model\n",
    "clf_model = DummyClassifier(strategy=\"most_frequent\") \n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', clf_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "827f3892-8a7b-4c7d-bb19-02fc83de2757",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'price_gt_1M'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/utils/__init__.py:448\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 448\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m all_columns\u001b[38;5;241m.\u001b[39mget_loc(col)\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'price_gt_1M'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[167], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf\u001b[38;5;241m.\u001b[39mscore(X_train,\u001b[38;5;250m \u001b[39my_train)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclf\u001b[38;5;241m.\u001b[39mscore(X_test,\u001b[38;5;250m \u001b[39my_test)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    360\u001b[0m     cloned_transformer,\n\u001b[1;32m    361\u001b[0m     X,\n\u001b[1;32m    362\u001b[0m     y,\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    364\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    365\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/joblib/memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m--> 724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[1;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[0;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m _get_column_indices(X, columns)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[0;32m~/anaconda3/envs/aap/lib/python3.11/site-packages/sklearn/utils/__init__.py:456\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    453\u001b[0m             column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training score: {clf.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test score: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70793e38-4f88-4777-82a7-7db804c1a77d",
   "metadata": {},
   "source": [
    "### Task 6 - a little error exploration (more challenging)\n",
    "\n",
    "This will challenge your pandas and your general data manipulation skills. Just give it your best shot. If you don't get, you don't get it. It doesn't require much code - just the right code. :)\n",
    "\n",
    "I also include another data file called `kc_house_data_regression.csv` in which the target variable is `price`. Everything else is exactly the same, including the order of the rows. So, here's your challenge. Using Model 2 (the lasso model with `C=1.0`), start by using the `predict` method to generate an array of predictions for the original test data. Obviously, some of the predictions are correct and some of them are not. It would be interesting to know more about the kinds of errors our model is making. We can see some things from the confusion matrix. However, since we don't have the actual `price` value, it's hard to visualize how the errors relate to it. For example, are we only making errors when the price is really close to 1 million? One way to visualize this is to create a histogram of the actual prices **only for those rows in test that we got wrong**. What makes this tricky? A few things:\n",
    "\n",
    "* As I already mentioned, `price` is not in our original data but is in the `kc_house_data_regression.csv` dataset. Remember, other than the target variable, this dataset is identical (including the index) to the one we used above for classification.\n",
    "* We partitioned the classification dataset into training and test datasets.\n",
    "* In order to create the histogram, we simply need a Series (or array) of `price` values corresponding to the predictions in test that we got wrong.\n",
    "\n",
    "**HINTS** \n",
    "\n",
    "* The pandas `join` method will come in handy.\n",
    "* The pandas `.loc` selector can take a boolean array as an input for selecting rows or columns. Using one to select rows is quite useful for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026c076",
   "metadata": {},
   "source": [
    "## Optional Hacker Extra Credit tasks\n",
    "I always like to include some extra credit tasks for those who want to push themselves a little further. For this problem, consider doing one or more of the following:\n",
    "\n",
    "* Try out the [Histogram based Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html?highlight=histogram%20based%20gradient%20boosting%20classifier) shown in the optional materials at the end of Module 2. Compare its performance to logistic regression and the random forest.\n",
    "* I also include another data file called `kc_house_data_regression.csv` in which the target variable is `price`. Use sklearn's `LassoCV` to find a good regression model for predicting `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32e527",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "You should simply compress your entire project folder as either a zip file or a tar.gz file (do **NOT** ever use WinRAR to create rar files in this class). Note that when you do this, your \"hidden\" `.git` folder will get included. So, I'll be able to tell that you put the project under version control and I'll be able to look at your project folder structure. Before compressing the project folder to submit it:\n",
    "\n",
    "* make sure all of your notebooks and other files are in the main project folder and have good filenames,\n",
    "* make sure you've committed all of your changes (git),\n",
    "* upload your compressed folder in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0ad2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aap]",
   "language": "python",
   "name": "conda-env-aap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
